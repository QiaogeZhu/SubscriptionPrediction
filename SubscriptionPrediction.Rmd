---
title: "Subscription Prediction"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

# Data Explanation

This dataset has been dowloaded from UCI Machine Learning Repository. "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed." (See https://archive.ics.uci.edu/ml/datasets/bank+marketing). See also the `dataset_description.txt` and `attribute_info.txt` for explanation of the datasets and variables.

# Data Preparation

```{r warning=FALSE, message=FALSE}
# libraries
library(dplyr)
library(naniar)
library(visdat)
library(mice)
library(ggplot2)
library(corrplot)
library(e1071)
library(plotly)
```

```{r}
# read the data
data = read.csv("bank-additional/bank-additional-full.csv", sep = ";")
```

Take a look at the data:

```{r}
summary(data)
```

```{r}
str(data)
```

There are 21 variables and 41,188 observations in total. The types of our features are diverse.  
- numeric variables: `age`, `duration`, `campaign`, `pdays`(discrete), `previous`(discrete), `emp.var.rate`, `cons.price.idx`, `cons.conf.idx`, `euribor3m`, `nr.employed`  
- categorical variables: `job`, `marital`, `education`, `default`, `housing`, `loan`, `contact`, `month`, `day_of_week`, `poutcome`
- target variables: `y`

There are some variables with level **unknown**, which should be treated as missing values.  
Also, from the document `attribute_info.txt`, `pdays` (number of days that passed by after the client was last contacted from a previous campaign) has a value of 999 which means that client was not previously contacted. This variable is expected to be dealt with later regarding to the value of 999. 


## Missing value

```{r}
# number of missing values
cat("The number of missing values: ",sum(is.na(data)))
```

There is no "direct" missing value in the dataset. However, as we mentioned above, the entries marked as **unknown** should be treated as missing values. We replace all entries with **unknown** as NA.

```{r echo=FALSE}
data[data == "unknown"] = NA
```

After replacing all **unknown** values, let's plot the missing values.

```{r}
vis_miss(data, sort_miss = TRUE)
```

There are a few missing values associated with `job`, `marital`, `education`, `housing`, `loan`. But there are 20.87% missing values for `default`.

We use the `mice` package to impute the missing values.

```{r, eval = FALSE, echo=TRUE}
imputed_data = mice(data, maxit = 1, method = 'pmm', seed = 6)
data = complete(imputed_data)

# save the data
# write.csv(data, "cleaned_data_additional_full.csv")
```

```{r echo=TRUE}
# Now we can read the data directly
data = read.csv("cleaned_data_additional_full.csv")[,-1]
```

## Balance of Dataset 

In classification problem, the balance of dataset refers to the ratio of the number of output classes. Usually, 10:1 will be acceptable.  

```{r}
table(data$y)
cat("The ratio of balance: ", table(data$y)['no']/table(data$y)['yes'])
```

The dataset is balanced as the ratio is approximately 8:1.  

## Variable Exploration

### age

How is `age` distributed and how is it related to `y`?

```{r}
# distribution of variable age
ggplot(data, aes(x = age, color = y)) + 
  geom_histogram(fill = "white", alpha=0.8, position="identity") +
  ggtitle("The Distribution of Age") +
  xlab("Age") + 
  ylab("Count") +
  scale_color_brewer(palette = "Paired")
```

We can see that most of the potential subscribers are middle-aged. It also seems that even though most potential subscribers are middle-aged, the proportion of these people actually subscribed are comparably low. The proportion of young people and old people subscribing are high, but proportionally we do not have a lot of such customers.

### Job

How is `job` related to `y`?

```{r}
data %>% 
  group_by(y) %>%
  count(job) %>%
  ungroup() %>%
#ggplot(data, aes(fill=y, y=..count.., x=job)) + 
  #geom_bar(position="stack", stat="count") + 
  ggplot(aes(x=reorder(job,n),y=n,fill=y))+
  geom_bar(stat="identity")+
  ggtitle("How many observations for each type of jobs?") +
  xlab("Jobs") +
  ylab("Count") +
  coord_flip() +
  scale_fill_brewer(palette = "Paired")
```

Based on the plot, the proportion of retired and student subscribers are optimal. Most of people in our dataset are admin., blue-collar and technician, however, the proportion of subscribers is just a few. 

### marital status

```{r}
ggplot(data, aes(fill=y, y=..count.., x=marital)) + 
  geom_bar(position="stack", stat="count") +
  ggtitle("How many observations for each marital status?") +
  xlab("Marital Status") +
  ylab("Count") +
  scale_fill_brewer(palette = "Paired")
```

There are many married people but just a few of them are subscribers. 

### education

```{r}
data %>% 
  group_by(y) %>%
  count(education) %>%
  ungroup() %>%
  ggplot(aes(x=reorder(education,n),y=n,fill=y))+
    geom_bar(stat="identity") +
    ggtitle("How many observations for each type of education?") +
    xlab("Education") +
    ylab("Count") +
    coord_flip() +
    scale_fill_brewer(palette = "Paired")
#ggplot(data, aes(fill=y, y=..count.., x=education)) + 
#    geom_bar(position="stack", stat="count") +
```

Most people are educated, and the majority obtained university degree. It is hard to say that higher-educated people are more likely to subscribe. 

### default

```{r}
data %>% 
  group_by(y) %>%
  count(default) %>%
  ungroup() %>%
  #ggplot(data, aes(fill=y, y=..count.., x=default)) + 
  #  geom_bar(position="stack", stat="count")
  ggplot(aes(x=reorder(default,-n),y=n,fill=y))+
    geom_bar(stat="identity") +
    ggtitle("How many observations having credit in default?") +
    xlab("Default") +
    ylab("Count") +
    scale_fill_brewer(palette = "Paired")
```

Looks like only few observations have `default` = yes. We look at these observations.

```{r}
# observations where default = yes
data$y[data$default == "yes"]
```

Most of the observations did not subscribe. But we cannot say there is any pattern, since this is the case within the whole population.

### contact

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=contact)) + 
#    geom_bar(position="stack", stat="count")
data %>% 
  group_by(y) %>%
  count(contact) %>%
  ungroup() %>%
  ggplot(aes(x=reorder(contact,-n),y=n,fill=y))+
    geom_bar(stat="identity") +
    ggtitle("How many observations for each type of contact?") +
    xlab("Contact") +
    ylab("Count") +
    scale_fill_brewer(palette = "Paired")
```

It seems that people who are contacted by celluar are more likely to subscribe.

### housing

```{r}
data %>% 
  group_by(y) %>%
  count(housing) %>% 
  #ggplot(data, aes(fill=y, y=..count.., x=housing)) + 
  ggplot(aes(x=reorder(housing,-n), y=n, fill=y)) +
    geom_bar(stat="identity") +
    ggtitle("How many observations having housing loan?") +
    xlab("Housing") +
    ylab("Count") +
    scale_fill_brewer(palette = "Paired")
```

There aren't significant difference in the `housing` section.

### loan

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=loan)) + 
#    geom_bar(position="stack", stat="count")
data %>% 
  group_by(y) %>%
  count(loan) %>% 
  #ggplot(data, aes(fill=y, y=..count.., x=housing)) + 
  ggplot(aes(x=reorder(loan,-n), y=n, fill=y)) +
    geom_bar(stat="identity") +
    ggtitle("How many observations having personal loan?") +
    xlab("Loan") +
    ylab("Count") +
    scale_fill_brewer(palette = "Paired")
```

### month

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=month)) + 
#    geom_bar(position="stack", stat="count")
data %>% 
  group_by(y) %>%
  count(month) %>% 
  ggplot(aes(x=reorder(month,-n), y=n, fill=y)) +
    geom_bar(stat="identity") +
    ggtitle("How many observations for each month?") +
    xlab("Month") +
    ylab("Count") +
    scale_fill_brewer(palette = "Paired")
```

From the plot, we can see that there are many more observation in May. However, the proportion of people subscribed are not optimal. Also, there are not many observations on December, March, October, September, but the proportion of subscriptions is pretty high.

### day_of_week

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=day_of_week)) + 
#    geom_bar(position="stack", stat="count")
data %>% 
  group_by(y) %>%
  count(day_of_week) %>% 
  ggplot(aes(x=reorder(day_of_week,-n), y=n, fill=y)) +
    geom_bar(stat="identity") +
    ggtitle("How many observations for each last contact day of the week?") +
    xlab("Last contact day of the week") +
    ylab("Count") +
    scale_fill_brewer(palette = "Paired")
```

There aren't siginificant differences in `day_of_week`.

### duration

```{r, warning = FALSE, message=FALSE}
#ggplot(data, aes(fill=y, y=..count.., x=duration)) + 
#    geom_histogram()
ggplot(data, aes(x = duration, color = y)) + 
  geom_histogram(fill = "white", alpha=0.8, position="identity") +
  ggtitle("The Distribution of duration") +
  xlab("Duration") + 
  ylab("Count") +
  scale_color_brewer(palette = "Paired")
```

When duration is near 0, almost no one subscribed. However, as duration increases, it seems that the proportion of people who subscribed increases.

### campaign

```{r}
ggplot(data, aes(fill=y, y=..count.., x=campaign)) + 
  geom_bar(position="stack", stat="count") +
  scale_fill_brewer(palette = "Paired") +
  ggtitle("How many number of contacts performed during this campaign and for this client?") +
  xlab("Campaign") +
  ylab("Count")
```

### pdays

```{r}
# may be change level 999?
ggplot(data, aes(fill=y, y=..count.., x=pdays)) + 
  geom_bar(width = 100, position="stack", stat="count") +
  scale_fill_brewer(palette = "Paired") +
  ggtitle("How many number of days that passed by \nafter the client was last contacted from a previous campaign? \n (999 means client was not previously contacted)") +
  xlab("Number of Days") +
  ylab("Count")
```

Most of clients not previouly being contacted. Therefore, we plot again except the value of 999.  

```{r}
data %>% filter(pdays!=999) %>%
  ggplot(aes(fill=y, y=..count.., x=pdays)) + 
    geom_bar(position="stack", stat="count") +
    scale_fill_brewer(palette = "Paired") +
    ggtitle("How many number of days that passed by \nafter the client was last contacted from a previous campaign?") +
    xlab("Number of Days") +
    ylab("Count") +
    labs(caption="Note: The values of 999 have been discarded in this plot.")
```

Most of clients who were previously contacted were contacted less than 10 days ago. 

### previous

```{r, warning = FALSE}
ggplot(data, aes(fill=y, y=..count.., x=previous)) + 
    geom_bar(position="stack", stat="count") +
    scale_fill_brewer(palette = "Paired") +
    ggtitle("How many number of contacts performed before this campaign and for this client?") +
    xlab("Number of contacts") +
    ylab("Count")
```

### poutcome

```{r}
ggplot(data, aes(fill=y, y=..count.., x=poutcome)) + 
    geom_bar(position="stack", stat="count") +
    scale_fill_brewer(palette = "Paired") +
    ggtitle("Outcome of the previous marketing campaign") +
    xlab("Previous marketing campaign") +
    ylab("Count")
```

From the plot, we found that if previous marketing campagin succeeded, people are more likely to subscribe.

### emp.var.rate

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=emp.var.rate)) + 
#  geom_histogram(bins=6, position='stack') +
ggplot(data, aes(x=y,y=emp.var.rate,color=y)) +
  geom_boxplot() +
  scale_color_brewer(palette = "Paired") +
  ggtitle("The distribution of Employment variation rate (quarterly indicator)") + 
  ylab("Employment variation rate")
```

### cons.price.idx

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=cons.price.idx)) + 
#    geom_histogram(bins=5)
ggplot(data, aes(x=y,y=cons.price.idx,color=y)) +
  geom_boxplot() +
  scale_color_brewer(palette = "Paired") +
  ggtitle("The distribution of Consumer price index (monthly indicator)") + 
  ylab("Consumer price index")
```

### cons.conf.idx

```{r}
ggplot(data, aes(x=y,y=cons.conf.idx, color=y)) +
  geom_boxplot() +
  scale_color_brewer(palette = "Paired") +
  ggtitle("The distribution of Consumer Confidence Index (monthly indicator)") + 
  ylab("Consumer confidence index")

```

### euribor3m

```{r}
ggplot(data, aes(x=y,y=euribor3m, color=y)) +
  geom_boxplot() +
  scale_color_brewer(palette = "Paired") +
  ggtitle("The distribution of Euribor 3 month rate (daily indicator)") + 
  ylab("Euribor 3 month rate")
```

### nr.employed

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=nr.employed)) + 
#    geom_histogram(bins=4)

# distribution of variable age
ggplot(data, aes(x = nr.employed, color = y)) + 
  geom_histogram(fill="white",alpha=0.8, position="identity", bins=4) +
  ggtitle("The distribution of number of employees (quarterly indicator)") +
  xlab("Number of Employees") + 
  ylab("Count") +
  scale_color_brewer(palette = "Paired")
```

# Feature Engineering

## Correlation

We check the correlations of the variables.

```{r}
X = data %>%
  select(-y)

# change into integer
for(i in 1:ncol(X)){
  X[,i] <- as.integer(X[,i])
}


corr_mat=cor(X,method="s")
corrplot(corr_mat, is.corr = FALSE, win.asp = .7, method = "circle")
```

We denote that `emp.var.rate`, `euribor3m` and `nr.employed` are highly correlated to each other.

We look at the description of the three variables:

1. emp.var.rate: employment variation rate - quarterly indicator (numeric)

2. euribor3m: euribor 3 month rate - daily indicator (numeric)

3. nr.employed: number of employees - quarterly indicator (numeric)

We may consider remove `emp.var.rate` and `euribor3m` while modelling.

Also, we remove the `duration` variable based on the dataset author's note: "this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model."

```{r}
data = data %>%
  select(-c(emp.var.rate, euribor3m, duration))
```

# Modeling  

```{r echo=FALSE, message=FALSE}
library(caret)
library(catboost)
library(pROC)
library(ROCR)
library(xgboost)
library(cattonum)
library(mltools)
library(randomForest)
library(naivebayes)
```

```{r eval=FALSE}
# train-test split
set.seed(2383)
size = floor(0.75 * nrow(data))
train_index <- sample(seq_len(nrow(data)), size = size)
train = data[train_index,]
test = data[-train_index,]

# save the train and test dataset to avoid higher memory 
#write.csv(train, "train.csv")
#write.csv(test, "test.csv")
```

```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```


## Logistic  



## Naive Bayes 


```{r message=FALSE}
nb_control <- trainControl(
  method = "cv",
  number = 10,
  verbose = FALSE
)
nb_grid <- expand.grid(
  usekernel = TRUE,
  fL = 1,
  adjust = 5
)
nb_model <- train(
  x = train %>% select(-y),
  y = train$y,
  method = "nb",
  trControl = nb_control,
  tuneGrid = nb_grid,
  verbose = FALSE,
  preProc = c("center","scale")
)
```

```{r message=FALSE}
nb_pred = predict(nb_model, test)
nb_pred_acc = sum(nb_pred == test$y) / length(test$y)
cat("Prediction Accuracy Rate: ",nb_pred_acc)
```

```{r message=FALSE}
nb_roc <- roc(as.integer(test$y),as.integer(nb_pred),direction="<")
nb_auc <- auc(nb_roc)
plot(nb_roc, main=paste("ROC Curve (AUC Score: ",nb_auc,")"))
```


## SVM

```
svm_linear_control <- trainControl(
  method = "cv",
  number = 5,
  verbose = FALSE
)

svm_linear_grid <- expand.grid(
  C = c(0.1, 1, 10)
)

svm_linear_model <- train(
  y~.,
  data = train,
  method = "svmLinear",
  trControl = svm_linear_control,
  tuneGrid = svm_linear_grid,
  preProc = c("scale","center"),
  verbose = FALSE
)
svm_linear_model$bestTune
```

```
# radiao kernel 
```


## Random Forest

```{r, cache = TRUE}
rf_grid = expand.grid(mtry = 1 : 6)

oob = trainControl(method = "oob")
rf_oob_tune = train(
  y ~ .,
  data = train,
  method = "rf",
  trControl = oob,
  verbose = FALSE,
  tuneGrid = rf_grid
  )

rf_oob_tune$bestTune
```

```{r}
varImp(rf_oob_tune, scale = FALSE)
```

```{r}
rf_oob_pred = predict(rf_oob_tune, test)
rf_oob_pred_acc = sum(rf_oob_pred == test$y) / length(test$y)
cat("Prediction Accuracy Rate: ",rf_oob_pred_acc)
```

The best tune uses `mtry` = 3. By checking the variable importance, we can see that `nr.employed`, `cons.conf.idx`, and `cons.price.idx` are the most important variables for predicting whether a client subscribes to a term deposit or not. The prediction accuracy is **`r rf_oob_pred_acc`** on the test data.  

```{r message=FALSE}
rf_roc <- roc(as.integer(test$y),as.integer(rf_oob_pred),direction="<")
rf_auc <- auc(rf_roc)
plot(rf_roc, main=paste("ROC Curve (AUC Score: ",rf_auc,")"))
```

## XGBoost 

Before modeling, we handle the categorical and discrete variables first.  
For variables `job`, `marital`, `education`, `month`, `day_of_week`, `poutcome`, `pdays`, `campaign` and `previous`, we use mean encoding which calculates the proportion of each category for true label (y==1) and assigns it as the corresponding value.   
For `default`, `housing`, and `loan`, we use label encoding.  
Last we use one hot encoding for `contact`.  
The reason why we are using mean encoding for some categorical variables here is that mean encoding performs well on data which is high cardinality, which avoid too-high-dimensional problem in one hot encoding and inappropriate ranking information for nomial variables in label encoding. 

```{r eval=FALSE}
# mean encoding 
train_copy <- train[,]
mean_encoding <- function(dataset, categorical_col, response) {
  categorical_col <- enquo(categorical_col)
  response <- enquo(response)
  temp <- dataset %>% 
    group_by(!!categorical_col,!!response) %>% 
    summarise(n=n()) %>%
    mutate(freq=n/sum(n)) %>%
    filter(!!response=='yes') %>%
    select(c(!!categorical_col,freq))
  return(temp)
}

train_copy <- merge(train_copy,mean_encoding(train_copy,job,y),by='job') %>% select(-job) %>% mutate(job=freq) %>% select(-freq) 
train_copy <- merge(train_copy,mean_encoding(train_copy,marital,y),by='marital') %>% select(-marital) %>% mutate(marital=freq) %>% select(-freq) 
train_copy <- merge(train_copy,mean_encoding(train_copy,education,y),by='education') %>% select(-education) %>% mutate(education=freq) %>% select(-freq) 
train_copy <- merge(train_copy,mean_encoding(train_copy,month,y),by='month') %>% select(-month) %>% mutate(month=freq) %>% select(-freq) 
train_copy <- merge(train_copy,mean_encoding(train_copy,day_of_week,y),by='day_of_week') %>% select(-day_of_week) %>% mutate(day_of_week=freq) %>% select(-freq) 
train_copy <- merge(train_copy,mean_encoding(train_copy,poutcome,y),by='poutcome') %>% select(-poutcome) %>% mutate(poutcome=freq) %>% select(-freq) 
train_copy <- merge(train_copy,mean_encoding(train_copy,pdays,y),by='pdays') %>% select(-pdays) %>% mutate(pdays=freq) %>% select(-freq) 
train_copy <- merge(train_copy,mean_encoding(train_copy,campaign,y),by='campaign') %>% select(-campaign) %>% mutate(campaign=freq) %>% select(-freq) 
train_copy <- merge(train_copy,mean_encoding(train_copy,previous,y),by='previous') %>% select(-previous) %>% mutate(previous=freq) %>% select(-freq) 
```

```{r eval=FALSE}
# label encoding 
train_copy$default <- as.numeric(train_copy$default)-1
train_copy$housing <- as.numeric(train_copy$housing)-1
train_copy$loan <- as.numeric(train_copy$loan)-1
```

```{r eval=FALSE}
# one hot encoding
for (unique_value in unique(train_copy$contact)) {
  train_copy[paste("contact", unique_value, sep = ".")] <- ifelse(train_copy$contact == unique_value, 1, 0)
}
train_copy <- train_copy %>% select(-contact)
```

```{r eval=FALSE}
# test 
test_copy <- test[,]

test_copy <- merge(test_copy,mean_encoding(test_copy,job,y),by='job') %>% select(-job) %>% mutate(job=freq) %>% select(-freq) 
test_copy <- merge(test_copy,mean_encoding(test_copy,marital,y),by='marital') %>% select(-marital) %>% mutate(marital=freq) %>% select(-freq) 
test_copy <- merge(test_copy,mean_encoding(test_copy,education,y),by='education') %>% select(-education) %>% mutate(education=freq) %>% select(-freq) 
test_copy <- merge(test_copy,mean_encoding(test_copy,month,y),by='month') %>% select(-month) %>% mutate(month=freq) %>% select(-freq) 
test_copy <- merge(test_copy,mean_encoding(test_copy,day_of_week,y),by='day_of_week') %>% select(-day_of_week) %>% mutate(day_of_week=freq) %>% select(-freq) 
test_copy <- merge(test_copy,mean_encoding(test_copy,poutcome,y),by='poutcome') %>% select(-poutcome) %>% mutate(poutcome=freq) %>% select(-freq) 
test_copy <- merge(test_copy,mean_encoding(test_copy,pdays,y),by='pdays') %>% select(-pdays) %>% mutate(pdays=freq) %>% select(-freq) 
test_copy <- merge(test_copy,mean_encoding(test_copy,campaign,y),by='campaign') %>% select(-campaign) %>% mutate(campaign=freq) %>% select(-freq) 
test_copy <- merge(test_copy,mean_encoding(test_copy,previous,y),by='previous') %>% select(-previous) %>% mutate(previous=freq) %>% select(-freq) 

test_copy$default <- as.numeric(test_copy$default)-1
test_copy$housing <- as.numeric(test_copy$housing)-1
test_copy$loan <- as.numeric(test_copy$loan)-1

for (unique_value in unique(test_copy$contact)) {
  test_copy[paste("contact", unique_value, sep = ".")] <- ifelse(test_copy$contact == unique_value, 1, 0)
}
test_copy <- test_copy %>% select(-contact)
```

```{r eval=FALSE}
# save encoded dataset to avoid higher memory cost
#write.csv(train_copy, "encoded_train.csv")
#write.csv(test_copy, "encoded_test.csv")
```

```{r}
# read saved files instead  
train_copy <- read.csv("encoded_train.csv")
test_copy <- read.csv("encoded_test.csv")
```


Next, we are ready to build XGBoost model.  

```{r}
xgb_control <- trainControl(
  method = "cv",
  number = 5,
  search = "random",
  savePredictions = TRUE,
  classProbs = TRUE
)

xgb_grid <- expand.grid(
  eta=c(0.01, 0.1, 1),
  gamma=c(0.1, 1, 10),
  max_depth=c(3,4,6,8),
  colsample_bytree=c(0.5,0.7,0.9),
  nrounds=10,
  min_child_weight=c(2,6,10),
  subsample = c(0.7,0.9)
  )

xgb_model <- train(
  y~.,
  data = train_copy,
  method = "xgbTree",
  metric = "Accuracy",
  nthread = 2,
  verbosity = 0,
  tuneGrid = xgb_grid,
  trControl = xgb_control
)
xgb_model$bestTune
```

```{r}
xgb_pred <- predict(xgb_model, test_copy)
xgb_pred_acc <- sum(xgb_pred == test_copy$y) / length(test_copy$y)
cat("Prediction Accuracy Rate: ",xgb_pred_acc)
```

```{r}
varImp(xgb_model,scale = FALSE)
```

```{r message=FALSE}
xgb_roc <- roc(as.integer(test_copy$y),as.integer(xgb_pred),direction="<")
xgb_auc <- auc(xgb_roc)
plot(xgb_roc, main=paste("ROC Curve (AUC Score: ",xgb_auc,")"))
```


## CatBoost  

We notice that there are many categorical variables, and also some discrete variables which can also be treaded as categorical. CatBoost is a powerful model which performance on dataset with many categorical variables in that it has various methods to handle categorical variables. However, since some of categorical variables seem to be high cardinal, the result may not be as good as we expect.  

```{r}
# extract features and labels for modeling
features <- data %>% select(-c(y))
labels <- data$y
```

```{r message=FALSE}
cat_control <- trainControl(
  method = "cv",
  number = 5,
  search = "random",
  classProbs = TRUE
)
cat_grid <- expand.grid(
  depth = c(4,6,8),
  learning_rate = c(0.01, 0.1, 1),
  l2_leaf_reg = c(0.1,1,2,4,6),
  rsm = 0.95,
  border_count = 64,
  iterations = 10
  
)
cat_model <- train(
  x = features,
  y = labels,
  method = catboost.caret,
  metric = "Accuracy",
  bootstrap_type = 'Bayesian',
  maximize = TRUE,
  logging_level='Silent',
  tuneGrid = cat_grid,
  trControl = cat_control
)
cat_model$bestTune
```

```{r}
cat_pred <- predict(cat_model, test)
cat_pred_acc <- sum(cat_pred == test$y) / length(test$y)
cat("Prediction Accuracy Rate: ",cat_pred_acc)
```

```{r}
varImp(cat_model,scale = FALSE)
```

```{r, message=FALSE}
#table(test$y, cat_pred)
cat_roc <- roc(as.integer(test$y),as.integer(cat_pred),direction="<")
cat_auc <- auc(cat_roc)
plot(cat_roc, main=paste("ROC Curve (AUC Score: ",cat_auc,")"))
```

The prediction accuracy rate on our test dataset is **`r cat_pred_acc`**, and the AUC score is `r cat_auc`. Our best parameters selected from the grid search is also shown above. From our best model, `nr.employed` is the most important variable. 

# Results
